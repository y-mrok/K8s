# Kubernetes の目的
アプリケーションをコンテナの形で自動ホストし、必要な数のアプリケーションのインスタンスを簡単にデプロイし、アプリケーション内の異なるサービス間の通信を簡単に実現できるようにすること。

# Kubernetes のクラスタ
コンテナ形式でアプリケーションをホストする、オンプレミスまたはクラウド上の物理または仮想のノードセットで構成されている
## ワーカーノード
デプロしたコンテナを動作させるノード
- kubelet
- kube-proxy
- コンテナランタイムエンジン
## マスターノード（コントロールプレーン）
Kubernetes クラスターを管理するノード　＝　どのワーカーノード上でどのコンテナをデプロイするか管理するなどノードとコンテナを監視する
- ETCD クラスタ
- Kube Controller Manager
- kube-apiserver
- kube-scheduler
## etcd （ ETCD CLUSTER ）
- キーバリュー形式で情報を格納するデータベース
- ワーカーノード上にデプロイしたコンテナの情報などが含まれる
## kube-scheduler
- コンテナの情報に基づいてコンテナを配置するための適切なワーカーノードを特定する
## Controller-Manager
- Controller-Manager
- Node-controller
- Replication-Controller
## kube-apiserver
- クラスター内のすべてのオペレーションをオーケストレーションする役割を担っている
- 各種コントローラーがクラスタの状態を監視して必要に応じて変更したり、ワーカーノードがサーバーと通信したりするのに使用される
- 定期的に kubelet からステータスレポートを取得し、ワーカーノードやノード上のコンテナの状態を監視する
## コンテナランタイムエンジン
- コンテナを動かすためのソフトウェア（ Docker 等）
- コンテナランタイムエンジンは、マスターノードやワーカーノード等クラスタ内のすべてのノードにインストールする
## kubelet
- ワーカーノードで動作するエージェント
- kube-apiserver からの指示でワーカーノード上のコンテナをデプロイしたり破棄したりする
## kube-proxy サービス
ワーカーノード間の通信を受け持つ

# ETCD
## ETCD とは
シンプルで安全、かつ高速な分散型の信頼性の高いキーバリューストア
## キーバリューストア
- 情報をキーとバリュー（値）の形式で保存
- キーの値の重複は許されない
- 高速な読み書きが必要な設定データなど、小さなデータを保存・取得するのに適している
## ETCD をインストール
1. バイナリファイルをダウンロード
  ```
  curl -L https://github.com/etcd-io/etcd/releases/download/v3.4.20/etcd-v3.4.20-linux-arm64.tar.gz -o etcd-v3.4.20-linux-arm64.tar.gz
  ```
2. 展開
  ```
  tar xzvf etcd-v3.4.20-linux-arm64.tar.gz
  ```
3. ETCD を実行
  ```
  ./etcd
  ```
## 値の格納と取り出し
### 格納
```
./etcdctl set key1 value1
```
### 取り出し
```
./etcdctl get key1
```
## オプションを表示
```
./etcdctl
```
## Kubernetes における ETCD
- ノード、Pod、コンフィグ、シークレット、アカウント、ロール、バインディングなど、クラスタに関する情報を格納
- `kubectl get` コマンドを実行した時に表示される内容は、すべての ETCD の情報である
- ノートの追加、Pod やレプリカセットのデプロイなど、クラスタに加えたすべての変更を保持する
## Setup
### Manual
```
wget -q --https-only \
    "https://github.com/etcd-io/etcd/releases/download/v3.4.20/etcd-v3.4.20-linux-arm64.tar.gz"
```
### kubeadm
```
kubectl get pods -n kube-system
kubectl exec etcd-master -n kube-system etcdctl get / --prefix -key-only
```

## HA 環境
クラスタ内の複数のノードに分散して ETCD インスタンスが存在する状態

```
kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 
```

# ETCD - コマンド (オプション)

(オプション) ETCDCTLユーティリティに関する追加情報

ETCDCTL は ETCD と対話するために使用される CLI ツールです。

ETCDCTL は 2 つの API バージョン - Version 2 と Version 3 - を使って ETCD サーバと対話することができます。 デフォルトでは Version 2 を使用するように設定されています。それぞれのバージョンは異なるコマンドセットを持ちます。

例えば、ETCDCTL バージョン 2 は以下のコマンドをサポートしています。  
```
etcdctl backup
etcdctl cluster-health
etcdctl mk
etcdctl mkdir
etcdctl set
```

バージョン 3 ではコマンドは異なりますが
```
etcdctl snapshot save 
etcdctl endpoint health
etcdctl get
etcdctl put
```


正しいバージョンの API を設定するために、環境変数 ETCDCTL_API コマンドを設定します。
```
export ETCDCTL_API=3
```

APIバージョンが設定されていない場合、バージョン2が設定されているとみなされます。また、上記のバージョン3のコマンドは動作しません。APIバージョンがバージョン3に設定されている場合、上記のバージョン2のコマンドは動作しません。


これとは別に、ETCDCTLがETCD API Serverを認証できるように、証明書ファイルのパスを指定する必要があります。証明書ファイルはetcd-masterに以下のパスで用意されています。証明書については、このコースのセキュリティのセクションで詳しく説明します。ですから、これが複雑に見えても心配しないでください。
```
--cacert /etc/kubernetes/pki/etcd/ca.crt     
--cert /etc/kubernetes/pki/etcd/server.crt     
--key /etc/kubernetes/pki/etcd/server.key
```

そのため、前回の動画で紹介したコマンドを動作させるには、ETCDCTL APIのバージョンと証明書ファイルのパスを指定する必要があります。以下は最終形です。
```
kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key" 
```

# kube-apiserver
- kubernetes コントロールプレーンのフロントエンド
- `kibectl` コマンドは kube-apiserver と会話する
- ETCD データストアと通信する唯一のコンポーネント
- kube-scheduler , kube-controller-manager , kubelet などの他のコンポーネントは kube-apiserver を使用してクラスタ内を更新する
## Pod を作成する手順
1. kube-apiserver が Pod オブジェクトを作成
2. kube-apiserver が ETCD サーバーの情報を更新
3. kube-apiserver がユーザーに Pod が作成されたことを通知
4. kube-scheduler が kube-apiserver を継続的に監視し、ノードに割り当てられていない Pod を検知
5. kube-scheduler は Pod を配置するのに適切なノードを特定し、それを kube-apiserver に伝える
6. kube-apiserver が ETCD の情報を更新
7. kube-apiserver が適切なノードの kubelet に通知
8. 通知を受けた kubelet がノード上に Pod を作成
9. kubelete がコンテナランタイムにアプリケーションイメージのデプロイを指示
10. デプロイの終了後、kubelet が kube-apiserver にステータスを通知
11. kube-apiserver が ETCD のデータを更新
## kube-apiserver をインストール
```
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-apiserver
```
## View api-server - kubeadm
```
kubectl get pods -n kube-system
```
## View api-server options - kubeadm
```
cat /etc/kubernetes/manifests/kube-apiserver.yaml
```
## View api-server options
```
cat /etc/systemd/system/kube-apiserver.service
ps -aux | grep kube-apiserver
```

# kube-controller-manager
- Kubernetes の様々なコントローラを管理する
- システムなの様々なコンポーネントの状態を継続的に監視し、システム全体を望ましい機能状態にするために働くプロセス
  - ノードコントローラ  
    - ノードの状態を監視し、アプリケーションの実効を維持するために必要なくアクションを取る役割を担っている
    - kube-apiserver 経由で 5 秒間隔でノードの状態を監視する
    - ノードからのハートビートを監視し、ハートビートがなくなってから 40 秒経過後到達不能とマークする
    - ノードが復活しない場合、ノード上で動作していた Pod を他のノードに割り当てる
  - レプリケーションコントローラ
    - レプリカセットの状態を監視し、レプリカセット内で常に指定された数の Pod を利用できるようにする役割を担っている
    - 動作中の Pod がダウンした場合、新たな Pod を作り出す
  - 上記以外に多数のコントローラが存在する
    - Deployment
    - Namespace
    - Endopoint
    - Job
    - Service-Account
    - PV-Binder
    - PV-Protection 等
  - コントローラはすべて Kubernetes-controller-manager と呼ばれる単一のプロセスにパッケージ化されている
## Installing kube-controller-manager
```
wget https://storage.googleapis.com/kubernete-release/release/v.13.0/bin/linux/amd64/kube-controller-manager
```
## View kube-controller-manager - kubeadm
```
kubectl get pods -n kube-system
```
## View kube-controller-manager options - kubeadm
```
cat /etc/kubernetes/manifests/kube-controll-ermanager.yaml
```
## View controller-manager options
```
cat /etc/systemd/system/kube-controller-manager.service
```
## View controller-manager options
```
ps -aux | grep kube-controller-manager
```

# kube-scheduler
- どの Pod をどのノードに配置するかを決定する（実際にノード上に Pod を配置するのは Kubelet の役割）
- kube-scheduler がノードを選択する手順
  1. Pod のリソース要件などから、選択肢から不適切なノードを除外する
  2. 各ノードに Pod を配置したあとに空くであろうリソース量を元に、各ノードをランク付けする
  3. ランクの高いノードを Pod の配置先に決定する
## Installing kube-scheduler
```
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
```
## View kubernetes-scheduler options - kubeadm
```
cat /etc/kubernetes/manifests/kube-scheduler.yaml
```
## View kubernets-scheduler options
```
ps -aux | grep kube-scheduler
```

# kubelet
- kube-apiserver からの指示で kubernetes クラスタ上のノードに Pod を作成したり削除したりする
- 一定間隔でノードや Pod の状態を報告する
- Pod を作成する手順
  1. kube-apiserver が kubelet に Pod 作成の指示を出す
  2. 指示を受けた kubelet はコンテナランタイムエンジンに Pod イメージを引き出しインスタンスを実行するよう要求し、
  3. Pod とコンテナの状態の監視を続け、タイムリーに kube-apiserver に報告する
## Installing kubelet
```
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubelet
```
## View kubelet options
```
ps -aux | grep kubelet
```

# kube-proxy
- kube-proxy は Kubernetes クラスタ内の各ノードで実行されるプロセス
- kube-proxy はサービスを探し、新しいサービスを見つけると、そのサービスへのトラフィックをバックエンドの Pod に転送するための適切なルートを作成する
- ルートを作成する方法の 1 つが iptables ルールを使用すること
## Installing kube-proxy
```
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-proxy
```
## View kube-proxy - kubeadm
```
kubectl get pods -n kube-system
kubectl get deamonset -n kube-system
```

#  Recap - PODs
- コンテナは Pod と呼ばれる Kubernetes のオブジェクトにカプセル化されている
- Pod はアプリケーションの単一のインスタンスで、Kubernetes で作成できる最小のオブジェクト
- スケールアップするときは新しい Pod を追加し、スケールダウンするときは既存の Pod を削除する
  - Pod の負荷が増加した場合、クラスタ内のノードに同じ Pod をデプロイする
  - 負荷の増加の割合により、クラスタ内にノードを追加する場合もある
## Multi-Container PODs
- 複数のコンテナを持つ Pod のこと
- 同一 Pod 内のコンテナ同士で通信が可能
- マルチコンテナ Pod は「まれ」なケース
## kubectl
- Pod を作成し、nginx の docker イメージのインスタンスをデプロイする
```
kubectl run nginx --image nginx
```
- クラスタ内の Pod の一覧を確認する
```
kubectl get pods
```

# PODs with YAML
- トップレベルフィールド（必須フィールド）
  - apiVersion
    - Kubernetes API のバージョン
    - string
  - kind
    - 作成するオブジェクトの種類（ Pod , Service , Relicaset , Deployment ）
    - string
  - metadata
    - 名前（ name ）やラベル（ labels ）など、オブジェクトに関するデータ
    - dictionary
  - spec
    - オブジェクトに関する追加情報を Kubernetes に提供する
    - list / array
- apiVersion  

| Kind | Version |
| :--- | :--- |
| Pod | v1 |
| Service | v1 |
| ReplicaSet | apps/v1 |
| Deployment | apps/v1 |
- Pod を作成する
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: myapp-pod
    labbels:
      app: myapp
      type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  ```
  ```bash
  kubectl create -f pod-definition.yml
  ```
## コマンド
- 利用可能な Pod を一覧で確認
```
kubectl get pods
```
- Pod の詳細情報を確認
```
kubectl describe pod myapp-pod
```

# Recap - ReplicaSets
## レプリケーションコントローラ
- Kubernets クラスタ内で同一 Pods の複数のインスタンスを動作させ、高可用性を実現する
- 指定した数の Pod が常に動作している状態を保証するコントローラー
- Pod の利用者が増加した場合、Pod を追加して対応する　→　ロードバランス（負荷分散）
- Pod の利用者が増加し、かつ、Pod を増加しても追いつかない場合はノードを追加して対応する
- 現在、Replication Controller から Replica Set に移行しつつある
## レプリケーションコントローラ
### 定義
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end
spcec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: frontend
    spec:
      containers:
        - name: nginx-controller
          image: nginx
  replicas: 3
```
### コマンド
- ReplicationController を作成
```bash
kubectl create -f rc-definition.yml
```
- ReplicationController の一覧を表示
```
kubectl get replicationcontroller
```
- Pod の一覧を表示
```
kubectl get pods
```
## レプリカセット
### 定義
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-controller
          image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end
```
### コマンド
- replicaset を作成
```bash
kubectl create -f rc-definition.yml
```
- レプリカセットの一覧を表示
```
kubectl get replicaset
```
- Pod の一覧を表示
```
kubectl get pods
```
## Lables and Selectors
- レプリカセットは各 Pod を監視し、Pod に障害が発生した場合は新しい Pod をデプロイする
- `selector` の `matchLabels` に指定したラベルを使用して Pod をフィルタリングして監視対象 Pod を決定する
## Scale
- スケーリング　→　レプリカ数（動作していなければならない Pod の数）のこと
- スケーリングの変更は `replicas` の値を変更する
- レプリカ数を3 → 6 に変更 
  - YAML ファイル内の `replicas` の値を変更 
  ```yaml
  apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    name: myapp-replicaset
    labels:
      app: myapp
      type: front-end
  spec:
    template:
      metadata:
        name: myapp-pod
        labels:
          app: myapp
          type: front-end
      spec:
        containers:
          - name: nginx-controller
            image: nginx
    replicas: 3
    selector:
      matchLabels:
        type: front-end
  ```
  ```
  kubectl replace -f replicaset-definition.yml
  ```
  - YAML フォイルを更新せずにスケールを変更
  ```
  kubectl scale --replicas=6 -f replicaset-definition.yml
  ```
  - YAML フォイルを更新せずにスケールを変更
  ```
  kubectl scale --replicas=6 replicaset myapp-replicaset
  ```
## コマンド
- レプリカセットを作成
  ```
  kubectl crate -f replicaset-definition.yml
  ```
- 作成したレプリカセットの一覧を表示
  ```
  kubectl get replicaset
  ```
- レプリカセットを削除（レプリカセットを削除したタイミンgでえ
  ```
  kubectl delete replicaset myapp-replicaset
  ```
- 定義ファイルの変更を反映
  ```
  kubectl replace -f replicaset-definition.yml
  ```
  ```
  kubectl scale --replicas=6 -f replicaset-defyintion.yml
  ```

# Deployments
## ローリングアップデート
- レプリカセット内の Pod をアップデートする場合、稼働している Pod を順番にアップデートする方法
- Pod を順番にアップデートするため、サービス停止が発生しない
## 定義
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end
```
```
kubectl crate -f deployment-definition.yml
```
```
kubectl get deployments
```
```
kubectl get replicaset
```
```
kubectl get pods
```
## コマンド
- 作成したすべてのオブジェクトを一度に確認する
  ```
  kubectl get all
  ```
## Certifcation Tip!
ここでヒント!

すでにお気づきかもしれませんが、YAMLファイルを作成・編集するのは少し難しいです。特にCLIでは。試験中、ブラウザからターミナルにYAMLファイルをコピー＆ペーストするのは難しいと感じるかもしれません。`kubectl run`コマンドを使うことで、YAMLテンプレートを生成することができます。また、YAMLファイルを全く作成しなくても、`kubectl run`コマンドだけで済むこともあります。例えば、特定の名前とイメージを持つポッドやデプロイメントを作成するように言われた場合、単に`kubectl run`コマンドを実行するだけでよいのです。

以下のコマンドを使用して、前回の模擬テストに再挑戦してください。ただし、今回はYAMLファイルの代わりに以下のコマンドを使用してください。今後、すべての演習でできるだけこれらを使用するようにしてください。

参考文献 (このページを試験用にブックマークしてください。とても便利です)。  
https://kubernetes.io/docs/reference/kubectl/conventions/

**NGINX Pod の作成**
```
kubectl run nginx --image=nginx
```
**"-o yaml" オプションと "--dry-run" オプションを使用して POD ManifestのYAMLファイルを生成する**
```
kubectl run nginx --image=nginx --dry-run=client -o yaml
```
**Deployment を作成**
```
kubectl create deployment --image=nginx nginx
```
**"-o yaml" オプションと "--dry-run" オプションを使用して Deployment ManifestのYAMLファイルを生成する**
```
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml
```

**上記の方法で作成した Manifest に必要な変更を加えた後に実行する**
```
kubectl create -f nginx-deployment.yaml 
```

**レプリカセット内に 4 つの Pod を持つ Manifest を作成する例**
```
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml 
```

# Service
- アプリケーション内外の様々なコンポーネント間の通信を可能にする（ Pod 群間の接続を可能にする）
  - アプリケーションとユーザーを結びつける
  - アプリケーションと他のアプリケーションを結びつける
## サービスの種類
- NodePort Service  
  ノード上のポートを Listen し、そのポート上のリクエストをアプリケーションを実行している Pod のポートに転送する
- Cluster IP  
  クラスタ内に仮想 IP を作成し、フロントエンドサーバー群からバックエンドサーバー群など異なるサービス間の通信を可能にする
- LoadBalancer  
  サポートされているクラウドプロバイダーのサービスで提供されるロードバランサーを使用する
# NodePort Service
## ポート
- TargetPort  
  - Pod （サービス自体）に割り当てられたポート
  - Pod 自体は IP アドレスを持つ
- Port  
  - サービスに割り当てられたポート
  - サービスは独自の IP アドレスを持つ（サービスのクラスタ IP ）
- NodePort  
  - ノードに割り当てられたポート
  - ポート番号は 30000 ～ 32767 の範囲で設定する
  - ノード自体は IP アドレスを持つ
##  Pod （サービス）へのアクセス手順
1. NortPort にアクセス：ノードの IP アドレス + NortPort 番号でアクセス  
    ```
    curl 192.168.1.2:30008
    ```
2. サービスが NodePort へのリクエストを Port 経由で Pod の TargetPort に転送
3. Pod がリクエストを処理
## 作成
```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 80
      port: 80
      nodePort: 30008
  selector:
    app: myapp
    type: front-end
```
- ポートの定義内で必須なのは `port`
- `nodePort` を省略した場合、自動的に 30000 ～ 32767 内の空きポートを割り当てる
- `targetPort` と Pod の関連付けは `selector` で行う：上記のサービス内の `targetPort` は下記で定義される Pod に関連付けられる
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: fornt-end
spec:
  containeres:
    - name: nginx-container
      image: nginx
```
- サービスを作成
  ```
  kubectl create -f service-definition.yml
  ```
- サービスを確認：サービスのクラスター IP とポートマップを確認
  ```
  kubectl get services
  ```
- Pod にアクセス
  ```
  curl http://192.168.1.2:30008
  ```

# Cluster IP
- 各 Pod には IP アドレスが割り当てられている
  - レプリカセットの定義にもとづき適宜 Pod の破棄・作成が行われるため、Pod の IP アドレスに頼ったサービスの構築はできない
- フロントエンドの Pod 群とバックエンドの Pod 群のようなPod 群どうしの接続を確立するサービスが Cluster IP
## 作成
- `type: ClusterIP` は `type` のデフォルトの値である
- `targetport` 　バックエンド（転送する先）が公開されるポート
- `port` 　サービスが公開されるポート
- `selector` 　ポートと Pod を関連付ける 
```yaml
apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  ports:
    - targetPort: 80
      port: 80
  selector:
    app: myapp
    type: back-end
```
- サービスを作成
  ```
  kubectl create -f service-definition.yml
  ```
- サービスを確認：
  ```
  kubectl get service
  ```

# LoadBalancer
- 自前でロードバランサーを用意する
- クラウドプロバイダーが提供するのロードバランサーを使用する
  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: myapp-service
  spec:
    type: LoadBalancer
    ports:
      - targetPort: 80
        port: 80
        nodePort: 30008
  ```

# Namespeces
- Pod , Deployment , Service などのすべてのオブジェクトはネームスペース内にで作成される
- Kubernetes のセットアップ時に自動的に作成されるデフォルトのネームスペースが作成される
- 明示的にネームスペースを指定しない場合、デフォルトのネームスペースが使用される
- Kubernetes はクラスターを起動するとき、以下のネームスペースを起動する
  - default  
    - デフォルトのネームスペース
    - ネームスペースを指定しない場合、このネームスペース内にオブジェクトが作成される
  - kube-system
    - Kubernetes のシステムよって作成されたオブジェクトが格納されるネームsペース
  - kube-public 
    - 認証されていないユーザーを含むすべてのユーザーがアクセス可能なネームスペース
- ネームスペースごとに
  - ポリシーセットを持つことができる
  - リソースのクォーターを設定できる
## ネームスペースを指定したコマンド
- Pod の一覧を表示
  ```
  kubectl gt pods --namespace=kube-system
  ```
- オブジェクトを作成
  ```
  kubectl create -f pod-definition.yml --namespace=dev
  ```
- カレントのネームスペースを変更後、ネームスペース default の Pod の一覧を表示
  ```
  kubectl get pods --namespace=default
  ```
- すべてのネームスペースの Pod の一覧を確認
  ```
  kubectl get pods --all-namespaces
  ```
## ネームスペースを指定した定義
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  namespace: dev
  labels:
    app: myapp
    type: front-end
spec:
  container:
    - name: nginx-contrainer
      image: nginx
```
## ネームスペースを作成
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: dev
```
```
kubectl create -f namespace-dev.yml
```
- 定義ファイルを使用しないでネームスペースを作成
```
kubectl create namespace dev
``` 
## カレントのネームスペースを変更
```
kubectl config set-context $(kubectl config current-context) --namespace=dev
```
## リソースクォータ
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quote
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limit.cpu: "10"
    limites.memory: 10Gi
```
```
kubectl create -f compute-quota.yaml
```

# Imperative vs Declarative （命令型と宣言型）
## 命令的アプローチ
- 「どうやるか」を指定する方法
- 例 #1
  1. 「web-server」という名前でVMをプロビジョニングします。
  2. その上にNGINXソフトウェアをインストールします。
  3. ポート '8080' を使用するように設定ファイルを編集します。
  4. 設定ファイルを編集し、Webのパスを'/var/www/nginx'にします。
  5. GTI Repo -X から /var/www/nginx に Web ページをロードします。
  6. NGINXサーバーを起動します。
- 例 #2
  - Create Objects
    ```
    kubectl run --image=nginx nginx
    kubectl create deployment --image=nginx nginx
    kubectl expose deployment nginx --port 80
    ```
  - Update Objects
    ```
    kubectl edit deployment nginx
    kubectl scale deployment nginx --relicas=5
    kubectl set image deployment nginx nginx=nginx:1.18
    ```
  - その他
      ```
      kubectl create -f nginx.yaml
      kubectl replace -f nginx.yaml
      kubectl delete -f nginx.yaml
      ```
### 設定ファイルを使用した場合
#### Create Objects
- ローカルの定義ファイルをもとにオブジェクトを作成する
```
kubectl create -f nginx.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
  spec:
    containers:
      - name: nginx-container
      - image nginx
```
#### Update Objects
##### kubectl edit
- `kubectl edit` コマンドは Kubernetes のメモリ上の定義ファイルを編集する（ローカル上の定義ファイルは関係ない）
- 編集後の内容は即時反映される
``` 
kubectl edit deployment nginx
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx
status:
  conditions:
    - lastProbTime: null
      status: "True"
      type: Initialized
```
##### kubectl replace
- ローカル上の定義ファイルを変更し、その内容を反映させる
```
kubectl replace -f nginx.yaml
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end-service
spec:
  containers:
    - name: nginx-container
      image: nginx:1.18
```
既存のオブジェクトを削除してから変更を反映させる（オブジェクトを作り直す）
```
kubectl replace --force -f nginx.yaml
```

## 宣言的アプローチ
- 「なにをやるか」を指定（要件を宣言）する方法
- 例 #1
  ```
  VM Name: web-server
  Package: nginx
  Port: 8080
  Path: /var/www/nginx
  COde: GIT REPO -X
  ```
- 例 #2
  ```
  kubectl apply -f nginx.yaml
  ```
### 設定ファイルを使用した場合
#### Create Objects
- ローカルの定義ファイルをもとにオブジェクトを作成する
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: myapp-pod
    labels:
      app: myapp
      type: front-end-service
    spec:
      containers:
        - name: nginx-container
        - image nginx
  ```
- 単一の定義ファイル
  ```
  kubectl apply -f nginx.yaml
  ```
- 特定のディレクトリ内のすべての定義ファイルを対象
  ```
  kubectl apply -f /path/to/config-file 
  ```
#### Update Objects
- ローカル上の定義ファイルを変更し、その内容を反映させる
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: myapp-pod
    labels:
      app: myapp
      type: front-end-service
    spec:
      containers:
        - name: nginx-container
        - image nginx:1.18
  ```
  ```
  kubectl apply -f nginx.yaml
  ```

# 資格取得のヒント - Kubectlの命令型コマンド

定義ファイルを使って宣言的に作業することが多い中、命令型コマンドは1回限りの作業を素早く行うのに役立ち、また定義テンプレートを簡単に生成することができます。これにより、試験中の時間を大幅に短縮することができます。

その前に、以下のコマンドを使うときに便利な2つのオプションについて知っておいてください。

`-dry-run`  
デフォルトでは、コマンドを実行するとすぐにリソースが作成されます。単にコマンドをテストしたい場合は、`-dry-run=client`オプションを使用します。この場合、リソースは作成されませんが、リソースが作成可能かどうか、コマンドが正しいかどうかがわかります。

`-o yaml`  
これは、リソース定義をYAML形式で画面に出力します。


上記の2つを組み合わせて使用することで、リソース定義ファイルを素早く生成し、必要に応じて修正したりリソースを作成したりすることができます。


## POD

NGINXのPodを作成する
```
kubectl run nginx --image=nginx
```

実行せず(--dry-run)にPOD ManifestのYAMLファイルを生成します(-o yaml)
```
kubectl run nginx --image=nginx --dry-run=client -o yaml。
```

## Deployment

デプロイメントを作成する
```
kubectl create deployment --image=nginx nginx を実行します。
```

実行せず(--dry-run)にDeploymentのYAMLファイルを生成する(-o yaml)
```
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml。
```

4つのレプリカを持つデプロイメントを生成する
```
kubectl create deployment nginx --image=nginx --replicas=4
```

kubectl scale コマンドでデプロイメントをスケールすることも可能です。
```
kubectl scale deployment nginx --replicas=4
```
また、YAML の定義をファイルに保存して、以下のように修正する方法もあります。
```
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
```

その後、デプロイメントを作成する前に、レプリカやその他のフィールドで YAML ファイルを更新することができます。


## Service

redis-serviceというClusterIP型のServiceを作成し、redisポッドを6379番ポートで公開します。
```
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
```
(これは自動的にポッドのラベルをセレクタとして使用します)

または
```
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
```
(これはポッドのラベルをセレクタとして使用せず、代わりにセレクタを **app=redis** と仮定します. セレクタをオプションで渡すことはできません。そのため、ポッドのラベルセットが異なる場合、あまりうまく動作しません。そのため、サービスを作成する前にファイルを生成し、セレクタを修正する必要があります。）


NodePort型でnginxというServiceを作成し、ポッドnginxのポート80をノード上のポート30080に公開します。
```
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
```
(これにより、自動的にポッドのラベルがセレクタとして使用されますが、ノードポートを指定することはできません。ポッドでサービスを作成する前に、定義ファイルを生成して、ノードポートを手動で追加する必要があります)。

または
```
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml 
```
とします。
(この場合、Podsのラベルはセレクタとして使用されません）。

上記2つのコマンドにはそれぞれ課題があります。一方はセレクタを受け付けず、もう一方はノードポートを受け付けないのです。私は、`kubectl expose`コマンドを使うことをお勧めします。ノードポートを指定する必要がある場合は、同コマンドで定義ファイルを生成し、ノードポートを手入力してからサービスを作成します。

## 参考までに

https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands  
https://kubernetes.io/docs/reference/kubectl/conventions/
